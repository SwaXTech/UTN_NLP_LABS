{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnlpconda58a66029c51549b7b2d39520295f4ea7",
   "display_name": "Python 3.8.5 64-bit ('nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Laboratorio 3.\n",
    "\n",
    "## Procesando texto usando Scikit-Learn\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Procesamineto de texto básico con SciKit Learn\n",
    "\n",
    "SciKit Learn es una biblioteca de Python de código abierto para el aprendizaje de máquinas que viene con instalaciones básicas para el procesamiento de texto para apoyar el agrupamiento y clasificación, incluyendo tokenización, conteo de palabras y stemming (Obtener la forma raíz de las palabras).\n",
    "\n",
    "En este lab práctico vamos a revisar brevemente cómo utilizar Scikit Learn en Python y luego observar con más detalle en las instalaciones de procesamiento previo.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### El uso de [Scikit Learn](http://scikit-learn.org/)\n",
    "\n",
    "- [Github](https://github.com/scikit-learn/scikit-learn)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "Ejemplo de uso:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1,2],[2,3],[3,4],[4,5]])\n",
    "x = data[:,0]\n",
    "y = data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-10-31T18:55:22.160481</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 372.103125 248.518125 \nL 372.103125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \nL 364.903125 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"ma1d1a88c32\" style=\"stroke:#1f77b4;\"/>\n    </defs>\n    <g clip-path=\"url(#p970bb73bd4)\">\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"45.321307\" xlink:href=\"#ma1d1a88c32\" y=\"214.756364\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"146.775852\" xlink:href=\"#ma1d1a88c32\" y=\"148.865455\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"248.230398\" xlink:href=\"#ma1d1a88c32\" y=\"82.974545\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"349.684943\" xlink:href=\"#ma1d1a88c32\" y=\"17.083636\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p970bb73bd4)\" d=\"M 45.321307 224.64 \nL 45.321307 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m3ae040922c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#m3ae040922c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 1.0 -->\n      <g transform=\"translate(37.369744 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p970bb73bd4)\" d=\"M 96.04858 224.64 \nL 96.04858 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.04858\" xlink:href=\"#m3ae040922c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1.5 -->\n      <g transform=\"translate(88.097017 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p970bb73bd4)\" d=\"M 146.775852 224.64 \nL 146.775852 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.775852\" xlink:href=\"#m3ae040922c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2.0 -->\n      <g transform=\"translate(138.82429 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p970bb73bd4)\" d=\"M 197.503125 224.64 \nL 197.503125 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"197.503125\" xlink:href=\"#m3ae040922c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 2.5 -->\n      <g transform=\"translate(189.551563 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p970bb73bd4)\" d=\"M 248.230398 224.64 \nL 248.230398 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.230398\" xlink:href=\"#m3ae040922c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 3.0 -->\n      <g transform=\"translate(240.278835 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p970bb73bd4)\" d=\"M 298.95767 224.64 \nL 298.95767 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"298.95767\" xlink:href=\"#m3ae040922c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 3.5 -->\n      <g transform=\"translate(291.006108 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p970bb73bd4)\" d=\"M 349.684943 224.64 \nL 349.684943 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"349.684943\" xlink:href=\"#m3ae040922c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 4.0 -->\n      <g transform=\"translate(341.733381 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p970bb73bd4)\" d=\"M 30.103125 214.756364 \nL 364.903125 214.756364 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m6b87720154\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m6b87720154\" y=\"214.756364\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 2.0 -->\n      <g transform=\"translate(7.2 218.555582)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p970bb73bd4)\" d=\"M 30.103125 181.810909 \nL 364.903125 181.810909 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m6b87720154\" y=\"181.810909\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 2.5 -->\n      <g transform=\"translate(7.2 185.610128)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p970bb73bd4)\" d=\"M 30.103125 148.865455 \nL 364.903125 148.865455 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m6b87720154\" y=\"148.865455\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 3.0 -->\n      <g transform=\"translate(7.2 152.664673)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p970bb73bd4)\" d=\"M 30.103125 115.92 \nL 364.903125 115.92 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m6b87720154\" y=\"115.92\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 3.5 -->\n      <g transform=\"translate(7.2 119.719219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p970bb73bd4)\" d=\"M 30.103125 82.974545 \nL 364.903125 82.974545 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m6b87720154\" y=\"82.974545\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 4.0 -->\n      <g transform=\"translate(7.2 86.773764)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p970bb73bd4)\" d=\"M 30.103125 50.029091 \nL 364.903125 50.029091 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m6b87720154\" y=\"50.029091\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 4.5 -->\n      <g transform=\"translate(7.2 53.82831)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p970bb73bd4)\" d=\"M 30.103125 17.083636 \nL 364.903125 17.083636 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m6b87720154\" y=\"17.083636\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 5.0 -->\n      <g transform=\"translate(7.2 20.882855)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 224.64 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 364.903125 224.64 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 224.64 \nL 364.903125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 7.2 \nL 364.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p970bb73bd4\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.scatter(x,y)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Preprocesamiento de Texto con Scikit Learn\n",
    "\n",
    "Muchas de las aplicaciones de análisis de texto que vamos a considerar requerir tomar un texto (ej: un post), tokenizar, y utilizando los tokens como features, posiblemente después de la eliminación de palabras con lematización/stop words. Con Scikit Learn no necesitamos escribir código para hacer eso; podemos usar la clase CountVectorizer en su lugar. Una instancia de la clase se crea de la siguiente manera:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 1)"
   ]
  },
  {
   "source": [
    "Una vez que hemos creado la instancia, podemos utilizarlo para extraer una bolsa de palabras.\n",
    "\n",
    "La representación de una colección de documentos utilizando el método de Scikit Learn `fit_transform`. En este primer ejemplo de prueba, usamos una lista de cadenas como documentos, de la siguiente manera:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [\"How to format my hard disk\", \"Hard disk format problems\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(content)"
   ]
  },
  {
   "source": [
    "`fit_transform` ha extraído siete características de los dos \"documentos\"; podemos ver esto con el método `get_feature_names()`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "source": [
    "Se puede ver cuántas veces cada una de estas siete features se produce en los dos documentos haciendo:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 0, 1, 0]])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "source": [
    "Tenga en cuenta que esta llamada devuelve una matriz de dos filas, una por cada documento. Cada fila de siete elementos. Cada elemento especifica el número de elementos de una determinada feature se produjo en ese documento:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "X.toarray()[0]"
   ]
  },
  {
   "source": [
    "Nos da el vector sólo para el primer documento que contiene todas las palabras elegidas como las features.\n",
    "\n",
    "`CountVectorizer` tiene una serie de opciones muy útiles, que se detallan en su [página](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.ht)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Veamos ahora cómo funciona esto con una verdadera colección de documentos.\n",
    "\n",
    "Vamos a utilizar los datos del dataset '20 Newsgroup' que es una colección de alrededor de 20000 documentos procedentes de 20 grupos de noticias diferentes, que se utiliza comúnmente en experimentos de clasificacion de texto y la agrupación de texto.\n",
    "\n",
    "- [Set de datos](http://qwone.com/~jason/20Newsgroups/)\n",
    "\n",
    "Pero ya está incluido en Scikit Learn y se puede cargar haciendo:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "source": [
    "Para acelerar las cosas, en el resto del laboratorio sólo utilizaremos un subconjunto de los documentos, los que pertenecen a las siguientes 4 categorías."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
   ]
  },
  {
   "source": [
    "Podemos importar los documentos pertenecientes a las categorías de la siguiente manera:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset = 'train', categories = categories, shuffle=True, random_state = 42)"
   ]
  },
  {
   "source": [
    "Los archivos han sido cargados en el atributo data del objeto twenty_train\n",
    "Vamos ahora a crear un nuevo objeto CountVectorizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "source": [
    "Una vez más la función `fit_transform` se puede utilizar para tokenizar cada documento, identificar las palabras más relevantes, construir un diccionario de tales palabras, y crear para cada documento una representación vectorial en el que las palabras son las features y el valor de estas features es el número de ocurrencias de cada palabra en un documento.\n",
    "\n",
    "Al igual que en ejemplo de la prueba anterior:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts = vectorizer.fit_transform(twenty_train.data)"
   ]
  },
  {
   "source": [
    "Por ejemplo, si ahora queremos ver la frecuencia de la palabra 'algorithm' en 20NewGroups:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('algorithm')"
   ]
  },
  {
   "source": [
    "Para ver cuántos términos fueron extraidos, podemos utilizar get_feature_names() que hemos visto anteriormente:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "35788"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "source": [
    "La clase `CountVectorizer` de Scikit Learn puede hacer más procesamiento previo de una colección de documentos que simples tokenizaciones.\n",
    "Una importante etapa de preprocesamiento adicional de que la clase puede llevar a cabo es la eliminación de stop_words\n",
    "\n",
    "Esto se puede hacer mediante la especificación de un parámetro:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "source": [
    "Para ver qué palabras son stop words hacé lo siguiente:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst']"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "sorted(vectorizer.get_stop_words())[:20]"
   ]
  },
  {
   "source": [
    "Para hacer stemming (tener la palabra raíz) y un pre procesamiento más avanzado, necesitamos complementar Scikit Learn con NLTK"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Preprocesamiento más avanzado con NLTK"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "[NLTK](http://www.nltk.org/book/) es compatible con la mayoría de los tipos de procesamiento previo. También viene con varios recursos útiles como corpus y el léxico.\n",
    "\n",
    "El stemming en NLTK incluye implementaciones de varios algoritmos muy conocidos y utilizados, incluyendo el Porter Stemmer y el Lancaster Stemmer.\n",
    "\n",
    "Para crear un stemmer en ingles se tiene que hacer lo siguiente:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "s = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "source": [
    "Después de crear el steammer, a continuación, puede utilizarlo para llevar a la raíz, palabras de la siguiente manera:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "s.stem('cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "s.stem('loving')"
   ]
  },
  {
   "source": [
    "Otros tipos de pre-procesamiento de NLTK incluye implementaciones de muchos de los módulos de procesamiento previo y analizadores sintácticos que discutimos o discutiremos en clase:\n",
    "\n",
    "- Identificadores de idioma\n",
    "- Tokenizers para varios idiomas\n",
    "- Divisores de oraciones\n",
    "- POS tagger\n",
    "- Chunkers\n",
    "- Parsers\n",
    "\n",
    "Además NLTK incluye implementaciones de los aspectos del análisis de texto que vamos a discutir en este módulo, incluyendo:\n",
    "\n",
    "- NER\n",
    "- Análisis de los sentimientos\n",
    "- Extraer información de los medios de redes sociales\n",
    "\n",
    "Por ejemplo, las instrucciones siguientes:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/swaxtech/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "text = word_tokenize('And now for something completely different')"
   ]
  },
  {
   "source": [
    "producir una versión tokenizada de la frase, que luego puede ser alimentado en el POS Tagger"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/swaxtech/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package maxent_treebank_pos_tagger to\n[nltk_data]     /home/swaxtech/nltk_data...\n[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n[nltk_data]       date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "source": [
    "## Integración del Stemmer de NLTK con el CountVectorizer de Scikit Learn\n",
    "\n",
    "El stemmer de NLTK puede ser utilizado antes de la alimentacion en CountVectorizer de Scikit Learn obteniendo así un índice más compacto.\n",
    "Una forma de hacer esto es definir una nueva clase StemmedCountVectorizer\n",
    "Extendiendo de CountVectorizer y redefiniendo el [metodo `build_analizer()`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) que se encarga de pre-procesamiento y tokenización:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['jhon', 'bought', 'carrots', 'potatoes']"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "analyze = vectorizer.build_analyzer()\n",
    "analyze('Jhon bought carrots and potatoes')"
   ]
  },
  {
   "source": [
    "Si modificamos `build_analyzer()` para aplicar el stemmer de NLTK a la salida del método `build_analyzer()`, obtenemos una versión que deriva así:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "source": [
    "Ahora podemos crear una instancia de nuestra clase:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_vectorizer = StemmedCountVectorizer(min_df=1, stop_words = 'english')\n",
    "stem_analyze = stem_vectorizer.build_analyzer()"
   ]
  },
  {
   "source": [
    "y = stem_analyze('Jhon bought carrots and potatoes')\n",
    "\n",
    "for tok in y:\n",
    "    print(tok)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "jhon\nbought\ncarrot\npotato\n"
     ]
    }
   ]
  },
  {
   "source": [
    "Si usamos este vectorizer para extraer features para el subconjunto del dataset 20_newsgroups que consideramos antes, vamos a tener un menor número de features:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26888"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism','soc.religion.christian','comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "train_counts = stem_vectorizer.fit_transform(twenty_train.data)\n",
    "len(stem_vectorizer.get_feature_names())"
   ]
  },
  {
   "source": [
    "# Implementación en español"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "spanish_stemmer = nltk.stem.SnowballStemmer('spanish')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (spanish_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "juan\ncompr\nzanahori\npap\n"
     ]
    }
   ],
   "source": [
    "stem_vectorizer = StemmedCountVectorizer(min_df=1)\n",
    "stem_analyze = stem_vectorizer.build_analyzer()\n",
    "\n",
    "y = stem_analyze('Juán compró zanahorias y papas')\n",
    "\n",
    "for tok in y:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package cess_esp to\n[nltk_data]     /home/swaxtech/nltk_data...\n[nltk_data]   Package cess_esp is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['El', 'grupo', 'estatal', 'Electricité_de_France', ...]"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('cess_esp')\n",
    "corpus = nltk.corpus.cess_esp.words()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14060"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "train_counts = stem_vectorizer.fit_transform(corpus)\n",
    "len(stem_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}